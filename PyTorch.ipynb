{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.FloatTensor(2,3)\n",
    "b=torch.FloatTensor([[1,2,3],[3,2,1]])\n",
    "c=torch.tensor(np.zeros(shape=(2,3)),dtype=torch.float32)\n",
    "d=torch.tensor([1,2,3])\n",
    "s=d.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag=torch.cuda.FloatTensor(2,3)\n",
    "bg=torch.cuda.FloatTensor([[1,2,3],[3,2,1]])\n",
    "cg=torch.cuda.FloatTensor(np.zeros(shape=(2,3)))\n",
    "dg=torch.cuda.IntTensor([1,2,3])\n",
    "sg=dg.sum()\n",
    "eg=torch.cuda.IntTensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU and GPU/CUDA transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=a.to(torch.device(\"cuda:0\")).to(torch.device(\"cpu\"))\n",
    "bc=b.to(\"cuda:0\").to(\"cpu\")\n",
    "cc=c.cuda(device=0).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False True True\n",
      "True True True False\n"
     ]
    }
   ],
   "source": [
    "v1=torch.tensor([1.,1.],requires_grad=True)\n",
    "v2=torch.tensor([2.,2.])\n",
    "v_sum=v1+v2\n",
    "v_res=(v_sum*2).sum()\n",
    "print(v_sum.is_leaf,v_res.is_leaf,v1.is_leaf,v2.is_leaf)\n",
    "print(v_sum.requires_grad,v_res.requires_grad,v1.requires_grad,v2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2.]), None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_res.backward()\n",
    "v1.grad,v2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5137, -0.1095],\n",
       "         [ 0.1600,  0.6351],\n",
       "         [-0.0566,  0.5130],\n",
       "         [ 0.4563, -0.4380],\n",
       "         [ 0.4440,  0.5241]]),\n",
       " tensor([-0.4219,  0.6126,  0.3535, -0.1582,  0.6087]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "l=nn.Linear(2,5)\n",
    "v=torch.FloatTensor([1,2])\n",
    "l.zero_grad()\n",
    "l(v)\n",
    "l.state_dict()['weight'], l.state_dict()[\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (5): Dropout(p=0.3)\n",
       "  (6): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=nn.Sequential(\n",
    "    nn.Linear(2,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Softmax(dim=1))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0947, 0.0869, 0.0947, 0.0945, 0.0831, 0.1181, 0.0866, 0.0947, 0.1450,\n",
       "         0.1018]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(torch.FloatTensor([[1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OurModule(\n",
       "   (pipe): Sequential(\n",
       "     (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=20, out_features=3, bias=True)\n",
       "     (5): Dropout(p=0.3)\n",
       "     (6): Softmax()\n",
       "   )\n",
       " ), tensor([0.2467, 0.4328, 0.3205], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OurModule(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_inputs,num_classes,dropout_prob=0.3):\n",
    "        super(OurModule,self).__init__()\n",
    "        self.pipe=nn.Sequential(\n",
    "            nn.Linear(num_inputs,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20,num_classes),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.pipe(x)\n",
    "    \n",
    "net=OurModule(num_inputs=2,num_classes=3)\n",
    "v=torch.FloatTensor([2,3])\n",
    "out=net(v)\n",
    "net, out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0502, 0.0500, 0.0496, 0.0504, 0.0496, 0.0500, 0.0500, 0.0510, 0.0500,\n",
       "        0.0502, 0.0492, 0.0500, 0.0512, 0.0494, 0.0502, 0.0498, 0.0498, 0.0500,\n",
       "        0.0495, 0.0500], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BigModule=nn.Sequential(\n",
    "    nn.Linear(2,3),\n",
    "    nn.ReLU(),\n",
    "    OurModule(3,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Softmax(dim=-1))\n",
    "BigModule.cuda(0)(torch.cuda.FloatTensor([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer=SummaryWriter(log_dir=\"test_run\")\n",
    "funcs={\"sin\":math.sin,\"cos\":math.cos,\"tan\":math.tan}\n",
    "\n",
    "for angle in range(-360,360):\n",
    "    angle_rad=angle*math.pi/180\n",
    "    for name, fun in funcs.items():\n",
    "        val=fun(angle_rad)\n",
    "        writer.add_scalar(name,val,angle)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
